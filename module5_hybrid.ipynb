{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5343d9f-7652-40b0-abf0-b8ea13c927b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dir: C:\\Users\\lenovo\\Desktop\\cs_module5_hybrid\n",
      "Chunks path: C:\\Users\\lenovo\\Desktop\\cs_module5_hybrid\\data_cscl\\chunks.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = Path.cwd()\n",
    "DATA_DIR = BASE_DIR / \"data_cscl\"\n",
    "CHUNKS_PATH = DATA_DIR / \"chunks.json\"\n",
    "\n",
    "print(\"Base dir:\", BASE_DIR)\n",
    "print(\"Chunks path:\", CHUNKS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343566c7-30ef-4366-98b0-760b6e54a140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1029\n",
      "Chunk keys: dict_keys(['paper_id', 'title', 'chunk_id', 'text'])\n"
     ]
    }
   ],
   "source": [
    "with open(CHUNKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    all_chunks = json.load(f)\n",
    "\n",
    "print(\"Total chunks:\", len(all_chunks))\n",
    "print(\"Chunk keys:\", all_chunks[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88568b26-4cf6-47c2-8a5a-8173798a78d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Local Embedding: 100%|█████████████████████████████████████████████████████████████████| 33/33 [00:24<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Desktop\\cs_module5_hybrid\\data_cscl\\embeddings_text3_small.npy (1029, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EMB_PATH = DATA_DIR / \"embeddings_text3_small.npy\"\n",
    "\n",
    "texts = [c[\"text\"] for c in all_chunks]\n",
    "print(\"Total chunks:\", len(texts))\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "emb_list = []\n",
    "\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Local Embedding\"):\n",
    "    batch = texts[i:i + BATCH_SIZE]\n",
    "    batch_emb = model.encode(batch, show_progress_bar=False, convert_to_numpy=True)\n",
    "    emb_list.append(batch_emb)\n",
    "\n",
    "embeddings = np.vstack(emb_list).astype(\"float32\")\n",
    "np.save(EMB_PATH, embeddings)\n",
    "\n",
    "print(EMB_PATH, embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fe3642-6a5a-4607-b60a-a48e5337adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: (1029, 384)\n",
      "Vectors in FAISS index: 1029\n",
      "FAISS index saved to: C:\\Users\\lenovo\\Desktop\\cs_module5_hybrid\\data_cscl\\faiss_index_text3_small.bin\n"
     ]
    }
   ],
   "source": [
    "embeddings = np.load(EMB_PATH).astype(\"float32\")\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # inner product index\n",
    "index.add(embeddings)\n",
    "\n",
    "print(\"Vectors in FAISS index:\", index.ntotal)\n",
    "\n",
    "FAISS_PATH = DATA_DIR / \"faiss_index_text3_small.bin\"\n",
    "faiss.write_index(index, str(FAISS_PATH))\n",
    "print(\"FAISS index saved to:\", FAISS_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b5a29a-9578-468a-81a3-7ed38ad4d905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created successfully!\n",
      "Location: C:\\Users\\lenovo\\Desktop\\cs_module5_hybrid\\data_cscl\\rag_hybrid.db\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = DATA_DIR / \"rag_hybrid.db\"\n",
    "\n",
    "# Remove existing DB if needed\n",
    "if DB_PATH.exists():\n",
    "    DB_PATH.unlink()\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.cursor()\n",
    "\n",
    "# Create papers table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE papers (\n",
    "    paper_id TEXT PRIMARY KEY,\n",
    "    title    TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Create chunks table\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE chunks (\n",
    "    chunk_id TEXT PRIMARY KEY,\n",
    "    paper_id TEXT,\n",
    "    text     TEXT\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert unique papers and chunks\n",
    "seen_papers = set()\n",
    "for c in all_chunks:\n",
    "    pid = c[\"paper_id\"]\n",
    "    if pid not in seen_papers:\n",
    "        cur.execute(\n",
    "            \"INSERT INTO papers (paper_id, title) VALUES (?, ?)\",\n",
    "            (pid, c[\"title\"])\n",
    "        )\n",
    "        seen_papers.add(pid)\n",
    "\n",
    "for c in all_chunks:\n",
    "    cur.execute(\n",
    "        \"INSERT INTO chunks (chunk_id, paper_id, text) VALUES (?, ?, ?)\",\n",
    "        (c[\"chunk_id\"], c[\"paper_id\"], c[\"text\"])\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(\"Database created successfully!\")\n",
    "print(\"Location:\", DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3e9dd01-9497-49bb-82f4-86fae3f3a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting term stats: 100%|█████████████████████████████████████████████████████| 1029/1029 [00:00<00:00, 5416.98it/s]\n",
      "Building BM25 table: 100%|█████████████████████████████████████████████████| 177245/177245 [00:00<00:00, 260462.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 inverted index built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenizer\n",
    "def simple_tokenize(text: str):\n",
    "    \"\"\"Simple tokenizer: lowercase, keep word characters only.\"\"\"\n",
    "    text = text.lower()\n",
    "    return re.findall(r\"\\b\\w+\\b\", text)\n",
    "\n",
    "# Collect term statistics from all chunks\n",
    "bm25_data = []\n",
    "doc_freq = Counter()\n",
    "doc_lengths = Counter()\n",
    "N = len(all_chunks)\n",
    "\n",
    "for c in tqdm(all_chunks, desc=\"Collecting term stats\"):\n",
    "    cid = c[\"chunk_id\"]\n",
    "    tokens = simple_tokenize(c[\"text\"])\n",
    "    doc_lengths[cid] = len(tokens)\n",
    "    term_counts = Counter(tokens)\n",
    "    for term, tf in term_counts.items():\n",
    "        doc_freq[term] += 1\n",
    "        bm25_data.append((term, cid, tf))\n",
    "\n",
    "avg_dl = sum(doc_lengths.values()) / N\n",
    "k1 = 1.5\n",
    "b = 0.75\n",
    "\n",
    "def bm25_idf(term):\n",
    "    df = doc_freq[term]\n",
    "    return np.log((N - df + 0.5) / (df + 0.5))\n",
    "\n",
    "# Add BM25 inverted index table\n",
    "cur.execute(\"DROP TABLE IF EXISTS bm25_inverted;\")\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE bm25_inverted (\n",
    "    term    TEXT,\n",
    "    chunk_id TEXT,\n",
    "    tf      INTEGER,\n",
    "    idf     REAL,\n",
    "    score   REAL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert rows\n",
    "for term, cid, tf in tqdm(bm25_data, desc=\"Building BM25 table\"):\n",
    "    dl = doc_lengths[cid]\n",
    "    idf = bm25_idf(term)\n",
    "    tf_norm = (tf * (k1 + 1)) / (tf + k1 * (1 - b + b * dl / avg_dl))\n",
    "    score = idf * tf_norm\n",
    "    cur.execute(\n",
    "        \"INSERT INTO bm25_inverted (term, chunk_id, tf, idf, score) VALUES (?, ?, ?, ?, ?)\",\n",
    "        (term, cid, tf, idf, score)\n",
    "    )\n",
    "\n",
    "conn.commit()\n",
    "print(\"BM25 inverted index built.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc144ac-13b6-4a45-940e-19df302a5197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector search using FAISS\n",
    "def vector_search(query: str, top_k: int = 10):\n",
    "    q_emb = model.encode([query], convert_to_numpy=True).astype(\"float32\")\n",
    "    faiss.normalize_L2(q_emb)\n",
    "    scores, idxs = index.search(q_emb, top_k)\n",
    "    idxs = idxs[0]\n",
    "    scores = scores[0]\n",
    "\n",
    "    results = []\n",
    "    for i, s in zip(idxs, scores):\n",
    "        c = all_chunks[int(i)]\n",
    "        results.append((c[\"chunk_id\"], float(s)))\n",
    "    return results\n",
    "\n",
    "# BM25 search using SQLite\n",
    "def bm25_search(query: str, top_k: int = 10):\n",
    "    \"\"\"BM25-like search using inverted index table.\"\"\"\n",
    "    terms = simple_tokenize(query)\n",
    "    placeholders = \",\".join(\"?\" * len(terms))\n",
    "    sql = f\"\"\"\n",
    "    SELECT chunk_id, SUM(tf * idf) AS score\n",
    "    FROM bm25_inverted\n",
    "    WHERE term IN ({placeholders})\n",
    "    GROUP BY chunk_id\n",
    "    ORDER BY score DESC\n",
    "    LIMIT ?;\n",
    "    \"\"\"\n",
    "    rows = cur.execute(sql, (*terms, top_k)).fetchall()\n",
    "    return [(cid, float(score)) for cid, score in rows]\n",
    "\n",
    "# Hybrid search\n",
    "def hybrid_search(query: str, top_k: int = 10, alpha: float = 0.5):\n",
    "    \"\"\"Hybrid search: BM25 + vector similarity.\"\"\"\n",
    "    bm25_raw = bm25_search(query, top_k=top_k * 5)\n",
    "    vec_raw = vector_search(query, top_k=top_k * 5)\n",
    "\n",
    "    bm25_scores = {cid: score for cid, score in bm25_raw}\n",
    "    vec_scores = {cid: score for cid, score in vec_raw}\n",
    "\n",
    "    all_ids = set(bm25_scores.keys()) | set(vec_scores.keys())\n",
    "    combined = []\n",
    "\n",
    "    for cid in all_ids:\n",
    "        s_bm25 = bm25_scores.get(cid, 0.0)\n",
    "        s_vec = vec_scores.get(cid, 0.0)\n",
    "        score = alpha * s_bm25 + (1 - alpha) * s_vec\n",
    "        combined.append((cid, score))\n",
    "\n",
    "    combined.sort(key=lambda x: x[1], reverse=True)\n",
    "    return combined[:top_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85e93892-03fd-4c6c-bf14-1a2ff56085bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector only hit-rate@5: 1.00 (5/5)\n",
      "Hybrid alpha=0.3 hit-rate@5: 1.00 (5/5)\n",
      "Hybrid alpha=0.5 hit-rate@5: 1.00 (5/5)\n",
      "Hybrid alpha=0.7 hit-rate@5: 1.00 (5/5)\n"
     ]
    }
   ],
   "source": [
    "# Simple manual queries for evaluation\n",
    "queries = [\n",
    "    \"What is speaker diarization?\",\n",
    "    \"How do neural models handle machine translation?\",\n",
    "    \"What are common approaches for sentiment analysis?\",\n",
    "    \"How is question answering evaluated?\",\n",
    "    \"What methods improve entity recognition?\",\n",
    "]\n",
    "\n",
    "def evaluate_search(search_fn, name: str, top_k: int = 5):\n",
    "    \"\"\"Approximate evaluation: how many queries hit at least one new paper.\"\"\"\n",
    "    total = len(queries)\n",
    "    hit = 0\n",
    "\n",
    "    for q in queries:\n",
    "        results = search_fn(q, top_k=top_k)\n",
    "        paper_ids = set()\n",
    "        for cid, _ in results:\n",
    "            # Map chunk_id back to paper_id\n",
    "            # all_chunks is a list; use a small map for speed if needed\n",
    "            for c in all_chunks:\n",
    "                if c[\"chunk_id\"] == cid:\n",
    "                    paper_ids.add(c[\"paper_id\"])\n",
    "                    break\n",
    "        if len(paper_ids) > 0:\n",
    "            hit += 1\n",
    "\n",
    "    print(f\"{name} hit-rate@{top_k}: {hit / total:.2f} ({hit}/{total})\")\n",
    "\n",
    "\n",
    "# Vector-only\n",
    "evaluate_search(vector_search, \"Vector only\", top_k=5)\n",
    "\n",
    "# Hybrid, different alpha\n",
    "for a in [0.3, 0.5, 0.7]:\n",
    "    eval_fn = lambda q, top_k=5, a=a: hybrid_search(q, top_k=top_k, alpha=a)\n",
    "    evaluate_search(eval_fn, f\"Hybrid alpha={a}\", top_k=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
